import yaml
import requests
from datetime import datetime
import pytz
import tldextract
import os
import json

# è„šæœ¬ç°åœ¨åœ¨ scripts/ æ–‡ä»¶å¤¹é‡Œè¿è¡Œï¼Œæ‰€æœ‰è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„éƒ½ç›¸å¯¹äº scripts/
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SOURCE_FILE = os.path.join(SCRIPT_DIR, "..", "config", "sources.yml")
OUTPUT_DNS = os.path.join(SCRIPT_DIR, "..", "adguard_dns.txt")       # è¾“å‡ºåˆ°æ ¹ç›®å½•
OUTPUT_README = os.path.join(SCRIPT_DIR, "..", "README.md")        # è¾“å‡ºåˆ°æ ¹ç›®å½•
STATS_FILE = os.path.join(SCRIPT_DIR, "..", "config", "ADGH_dns_stats.json")  # å­˜å‚¨åœ¨ config æ–‡ä»¶å¤¹

extractor = tldextract.TLDExtract(suffix_list_urls=None)


def normalize_domain(domain):
    domain = domain.strip().lower()
    domain = domain.lstrip("+.").lstrip("*.")
    ext = extractor(domain)
    if not ext.domain or not ext.suffix:
        return None
    return f"{ext.domain}.{ext.suffix}"


def fetch_domains(url):
    domains = set()
    print(f"[FETCH] {url}")
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        for line in r.text.splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if line.startswith("domain:"):
                raw = line.replace("domain:", "").strip()
            elif line.startswith("full:"):
                raw = line.replace("full:", "").strip()
            else:
                raw = line
            d = normalize_domain(raw)
            if d:
                domains.add(d)
    except Exception as e:
        print(f"[ERROR] fetch failed: {e}")
    print(f"[OK] got {len(domains)} domains")
    return domains


def chunk_list(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i + size]


def load_sources():
    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_stats():
    """è¯»å–ä¸Šæ¬¡è¿è¡Œçš„ç»Ÿè®¡æ•°æ®"""
    if not os.path.exists(STATS_FILE):
        return {}
    try:
        with open(STATS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[WARN] Failed to load stats file: {e}")
        return {}


def save_stats(stats):
    """ä¿å­˜æœ¬æ¬¡è¿è¡Œçš„ç»Ÿè®¡æ•°æ®"""
    try:
        with open(STATS_FILE, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[WARN] Failed to save stats file: {e}")


def generate_data(data):
    category_data = {}
    all_domains = set()
    stats = {}

    for category, cfg in data.get("categories", {}).items():
        dns = cfg.get("dns", "")
        urls = cfg.get("urls", [])
        raw_domains = set()

        for url in urls:
            raw_domains.update(fetch_domains(url))

        # ä¸å†è¿›è¡Œ DNS éªŒè¯ï¼Œæ‰€æœ‰åŸå§‹åŸŸåéƒ½è§†ä¸ºå­˜æ´»
        alive_domains = raw_domains

        category_data[category] = {
            "dns": dns,
            "domains": sorted(alive_domains),
            "raw_count": len(raw_domains),
            "alive_count": len(alive_domains),
        }
        stats[category] = len(alive_domains)
        all_domains.update(alive_domains)

        print(f"[FILTER] {category}: {len(raw_domains)} â†’ {len(alive_domains)}")

    return category_data, all_domains, stats


def write_dns(category_data):
    with open(OUTPUT_DNS, "w", encoding="utf-8") as f:
        # å®Œå…¨ä¸å†™ä»»ä½•å¤´éƒ¨æ³¨é‡Šï¼Œç›´æ¥å¼€å§‹å†™è§„åˆ™
        for category, info in category_data.items():
            domains = info["domains"]
            dns = info["dns"]
            if not domains:
                continue

            for chunk in chunk_list(domains, 200):  # æˆ–ä½ å–œæ¬¢çš„æ•°å­—ï¼Œæ¯”å¦‚ 60/100/200
                merged = "/".join(chunk)
                f.write(f"[/{merged}/]{dns}\n")

            f.write("\n")  # ç±»åˆ«é—´åŠ åˆ†éš”


def write_readme(all_domains, category_data, prev_stats):
    beijing = pytz.timezone("Asia/Shanghai")
    now = datetime.now(beijing).strftime("%Y-%m-%d %H:%M:%S")
    date_badge = datetime.now(beijing).strftime("%Y-%m-%d")

    total_count = len(all_domains)

    # æ„å»ºç»Ÿè®¡è¡¨æ ¼
    table_rows = []

    # è®¡ç®—ä¸Šä¸€æ¬¡çš„æ€»æ•°ï¼Œç”¨äºè¡¨æ ¼åº•éƒ¨å¯¹æ¯”
    prev_total = 0
    for cat, info in category_data.items():
        prev_total += prev_stats.get(cat, 0)

    for cat, info in category_data.items():
        current = info['alive_count']
        # è·å–ä¸Šæ¬¡æ•°é‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 0
        prev = prev_stats.get(cat, 0)

        # è®¡ç®—å˜åŒ–
        diff = current - prev

        # æ ¼å¼åŒ–å˜åŒ–æ˜¾ç¤ºï¼šå¸¦é¢œè‰²å’Œç®­å¤´
        if diff > 0:
            diff_str = f"ğŸ”¼ +{diff}"
        elif diff < 0:
            diff_str = f"ğŸ”½ {diff}"
        else:
            diff_str = "â– 0"

        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼ˆprev == 0 ä¸” current > 0ï¼‰ï¼Œå¯ä»¥æ ‡è®°ä¸º New
        if prev == 0 and current > 0:
            diff_str = "ğŸ†• New"

        table_rows.append(
            f"| {cat} | {prev:,} | {current:,} | {diff_str} |"
        )

    # è®¡ç®—æ€»å˜åŒ–
    total_diff = total_count - prev_total
    if total_diff > 0:
        total_diff_str = f"ğŸ”¼ +{total_diff}"
    elif total_diff < 0:
        total_diff_str = f"ğŸ”½ {total_diff}"
    else:
        total_diff_str = "â– 0"

    # æ€»è®¡è¡Œ
    table_rows.append(
        f"| **æ€»è®¡** | **{prev_total:,}** | **{total_count:,}** | **{total_diff_str}** |"
    )

    content = f"""# ğŸ›¡ï¸ AdGuardHome DNS åˆ†æµè§„åˆ™

![Total Domains](https://img.shields.io/badge/åŸŸåæ€»æ•°-{total_count}-blue?style=flat-square)
![Last Update](https://img.shields.io/badge/æœ€åæ›´æ–°-{date_badge}-green?style=flat-square)

> ğŸ¤– æœ¬æ–‡ä»¶ç”±è„šæœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œç”¨äº AdGuardHome çš„ DNS åˆ†æµé…ç½®ã€‚è„šæœ¬ä¼šå¯¹æ¯”ä¸Šæ¬¡ç”Ÿæˆçš„æ•°é‡ï¼Œæ˜¾ç¤ºåŸŸåå¢å‡æƒ…å†µã€‚

---

## ğŸ“Š æ•°æ®ç»Ÿè®¡

| åˆ†ç±» | ä¸Šæ¬¡æ›´æ–° | æœ¬æ¬¡æ›´æ–° | æ›´æ–°å˜åŒ– |
| :--- | :---: | :---: | :---: |
{chr(10).join(table_rows)}

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

1.  å¤åˆ¶ä»“åº“æ ¹ç›®å½•ä¸‹çš„ `adguard_dns.txt` æ–‡ä»¶å†…å®¹ã€‚
2.  æ‰“å¼€ AdGuardHome ç®¡ç†é¢æ¿ã€‚
3.  è¿›å…¥ **è®¾ç½®** -> **DNS æœåŠ¡**ã€‚
4.  åœ¨ **ä¸Šæ¸¸ DNS æœåŠ¡å™¨** é…ç½®ä¸­ï¼Œæ‰¾åˆ°æˆ–æ–°å»ºå¯¹åº”çš„æœåŠ¡å™¨è§„åˆ™ï¼ˆé€šå¸¸æ˜¯ç‰¹å®šåŸŸåçš„åˆ†æµï¼‰ã€‚
5.  å°†å†…å®¹ç²˜è´´å¹¶ä¿å­˜åº”ç”¨å³å¯ã€‚

---

## â° æ›´æ–°è®°å½•

- **ç”Ÿæˆæ—¶é—´**: {now} (åŒ—äº¬æ—¶é—´)
- **ç”Ÿæˆè„šæœ¬**: `scripts/ADGH_dns.py`
"""
    with open(OUTPUT_README, "w", encoding="utf-8") as f:
        f.write(content)


def main():
    print("=== Program start ===")

    # 1. è¯»å–ä¸Šæ¬¡çš„ç»Ÿè®¡æ•°æ®
    prev_stats = load_stats()
    print("[OK] Previous stats loaded")

    # 2. è¯»å–é…ç½®å¹¶ç”Ÿæˆæ•°æ®
    data = load_sources()
    print("[OK] sources.yml loaded")
    category_data, all_domains, stats = generate_data(data)

    # 3. ä¿å­˜è¿™æ¬¡çš„ç»Ÿè®¡æ•°æ®ï¼ˆä¾›ä¸‹æ¬¡å¯¹æ¯”ï¼‰
    save_stats(stats)
    print("[OK] Current stats saved")

    # 4. å†™å…¥ DNS æ–‡ä»¶å’Œ README
    write_dns(category_data)
    write_readme(all_domains, category_data, prev_stats)
    print(f"=== Done: {len(all_domains)} domains ===")


if __name__ == "__main__":
    main()
