import yaml
import requests
from datetime import datetime
import pytz
import tldextract
import os
import json

# è„šæœ¬ç°åœ¨åœ¨ scripts/ æ–‡ä»¶å¤¹é‡Œè¿è¡Œ
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SOURCE_FILE = os.path.join(SCRIPT_DIR, "..", "config", "sources.yml")
OUTPUT_DNS = os.path.join(SCRIPT_DIR, "..", "adguard_dns.txt")
OUTPUT_README = os.path.join(SCRIPT_DIR, "..", "README.md")
# ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
STATS_FILE = os.path.join(SCRIPT_DIR, "..", "config", "ADGH_dns_stats.json")

extractor = tldextract.TLDExtract(suffix_list_urls=None)


def normalize_domain(domain):
    domain = domain.strip().lower()
    domain = domain.lstrip("+.").lstrip("*.")
    ext = extractor(domain)
    if not ext.domain or not ext.suffix:
        return None
    return f"{ext.domain}.{ext.suffix}"


def fetch_domains(url):
    domains = set()
    print(f"[FETCH] {url}")
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        for line in r.text.splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if line.startswith("domain:"):
                raw = line.replace("domain:", "").strip()
            elif line.startswith("full:"):
                raw = line.replace("full:", "").strip()
            else:
                raw = line
            d = normalize_domain(raw)
            if d:
                domains.add(d)
    except Exception as e:
        print(f"[ERROR] fetch failed: {e}")
    print(f"[OK] got {len(domains)} domains")
    return domains


def chunk_list(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i + size]


def load_sources():
    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def load_stats():
    """è¯»å–ä¸Šæ¬¡è¿è¡Œçš„ç»Ÿè®¡æ•°æ®"""
    # æ‰“å°ç»å¯¹è·¯å¾„ï¼Œæ–¹ä¾¿è°ƒè¯•
    stats_abs_path = os.path.abspath(STATS_FILE)
    print(f"[INFO] Loading stats from: {stats_abs_path}")
    
    if not os.path.exists(STATS_FILE):
        print(f"[INFO] Stats file not found, starting fresh.")
        return {}
    try:
        with open(STATS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"[WARN] Failed to load stats file: {e}")
        return {}


def save_stats(stats):
    """ä¿å­˜æœ¬æ¬¡è¿è¡Œçš„ç»Ÿè®¡æ•°æ®"""
    try:
        stats_dir = os.path.dirname(STATS_FILE)
        stats_abs_path = os.path.abspath(STATS_FILE)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not os.path.exists(stats_dir):
            os.makedirs(stats_dir, exist_ok=True)
            print(f"[INFO] Created directory: {stats_dir}")
        else:
            print(f"[INFO] Directory already exists: {stats_dir}")
        
        # å†™å…¥æ–‡ä»¶
        with open(STATS_FILE, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„å†™æˆåŠŸäº†
        if os.path.exists(STATS_FILE):
            print(f"[INFO] Stats saved successfully to: {stats_abs_path}")
            print(f"[INFO] File size: {os.path.getsize(STATS_FILE)} bytes")
        else:
            print(f"[ERROR] File not found after save attempt: {stats_abs_path}")
    except Exception as e:
        print(f"[ERROR] Failed to save stats file: {e}")
        import traceback
        traceback.print_exc()


def generate_data(data):
    category_data = {}
    all_domains = set()
    stats = {}

    for category, cfg in data.get("categories", {}).items():
        dns = cfg.get("dns", "")
        urls = cfg.get("urls", [])
        raw_domains = set()

        for url in urls:
            raw_domains.update(fetch_domains(url))

        # ä¸å†è¿›è¡Œ DNS éªŒè¯ï¼Œæ‰€æœ‰åŸå§‹åŸŸåéƒ½è§†ä¸ºå­˜æ´»
        alive_domains = raw_domains  # ä¿®å¤ï¼šalive_domains è€Œä¸æ˜¯ alive_digits

        category_data[category] = {
            "dns": dns,
            "domains": sorted(alive_domains),
            "raw_count": len(raw_domains),
            "alive_count": len(alive_domains),
        }
        stats[category] = len(alive_domains)
        all_domains.update(alive_domains)

        print(f"[FILTER] {category}: {len(raw_domains)} â†’ {len(alive_domains)}")

    return category_data, all_domains, stats


def write_dns(category_data):
    with open(OUTPUT_DNS, "w", encoding="utf-8") as f:
        # å®Œå…¨ä¸å†™ä»»ä½•å¤´éƒ¨æ³¨é‡Šï¼Œç›´æ¥å¼€å§‹å†™è§„åˆ™
        for category, info in category_data.items():
            domains = info["domains"]
            dns = info["dns"]
            if not domains:
                continue

            for chunk in chunk_list(domains, 200):
                merged = "/".join(chunk)
                f.write(f"[/{merged}/]{dns}\n")

            f.write("\n")


def write_readme(all_domains, category_data, prev_stats):
    beijing = pytz.timezone("Asia/Shanghai")
    now = datetime.now(beijing).strftime("%Y-%m-%d %H:%M:%S")
    date_badge = datetime.now(beijing).strftime("%Y-%m-%d")

    total_count = len(all_domains)

    table_rows = []
    prev_total = 0
    for cat, info in category_data.items():
        prev_total += prev_stats.get(cat, 0)

    for cat, info in category_data.items():
        current = info['alive_count']
        prev = prev_stats.get(cat, 0)
        diff = current - prev

        if diff > 0:
            diff_str = f"ğŸ”¼ +{diff}"
        elif diff < 0:
            diff_str = f"ğŸ”½ {diff}"
        else:
            diff_str = "â– 0"

        if prev == 0 and current > 0:
            diff_str = "ğŸ†• New"

        table_rows.append(
            f"| {cat} | {prev:,} | {current:,} | {diff_str} |"
        )

    total_diff = total_count - prev_total
    if total_diff > 0:
        total_diff_str = f"ğŸ”¼ +{total_diff}"
    elif total_diff < 0:
        total_diff_str = f"ğŸ”½ {total_diff}"
    else:
        total_diff_str = "â– 0"

    table_rows.append(
        f"| **æ€»è®¡** | **{prev_total:,}** | **{total_count:,}** | **{total_diff_str}** |"
    )

    content = f"""# ğŸ›¡ï¸ AdGuardHome DNS åˆ†æµè§„åˆ™

![Total Domains](https://img.shields.io/badge/åŸŸåæ€»æ•°-{total_count}-blue?style=flat-square)
![Last Update](https://img.shields.io/badge/æœ€åæ›´æ–°-{date_badge}-green?style=flat-square)

> ğŸ¤– æœ¬æ–‡ä»¶ç”±è„šæœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œç”¨äº AdGuardHome çš„ DNS åˆ†æµé…ç½®ã€‚è„šæœ¬ä¼šå¯¹æ¯”ä¸Šæ¬¡ç”Ÿæˆçš„æ•°é‡ï¼Œæ˜¾ç¤ºåŸŸåå¢å‡æƒ…å†µã€‚

---

## ğŸ“Š æ•°æ®ç»Ÿè®¡

| åˆ†ç±» | ä¸Šæ¬¡æ›´æ–° | æœ¬æ¬¡æ›´æ–° | æ›´æ–°å˜åŒ– |
| :--- | :---: | :---: | :---: |
{chr(10).join(table_rows)}

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

1.  å¤åˆ¶ä»“åº“æ ¹ç›®å½•ä¸‹çš„ `adguard_dns.txt` æ–‡ä»¶å†…å®¹ã€‚
2.  æ‰“å¼€ AdGuardHome ç®¡ç†é¢æ¿ã€‚
3.  è¿›å…¥ **è®¾ç½®** -> **DNS æœåŠ¡**ã€‚
4.  åœ¨ **ä¸Šæ¸¸ DNS æœåŠ¡å™¨** é…ç½®ä¸­ï¼Œæ‰¾åˆ°æˆ–æ–°å»ºå¯¹åº”çš„æœåŠ¡å™¨è§„åˆ™ï¼ˆé€šå¸¸æ˜¯ç‰¹å®šåŸŸåçš„åˆ†æµï¼‰ã€‚
5.  å°†å†…å®¹ç²˜è´´å¹¶ä¿å­˜åº”ç”¨å³å¯ã€‚

---

## â° æ›´æ–°è®°å½•

- **ç”Ÿæˆæ—¶é—´**: {now} (åŒ—äº¬æ—¶é—´)
- **ç”Ÿæˆè„šæœ¬**: `scripts/ADGH_dns.py`
"""
    with open(OUTPUT_README, "w", encoding="utf-8") as f:
        f.write(content)


def main():
    print("=== Program start ===")

    # æ‰“å°å…³é”®è·¯å¾„ï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"[DEBUG] SCRIPT_DIR: {SCRIPT_DIR}")
    print(f"[DEBUG] STATS_FILE: {STATS_FILE}")
    print(f"[DEBUG] STATS_FILE absolute: {os.path.abspath(STATS_FILE)}")

    # 1. è¯»å–ä¸Šæ¬¡çš„ç»Ÿè®¡æ•°æ®
    prev_stats = load_stats()
    print("[OK] Previous stats loaded")

    # 2. è¯»å–é…ç½®å¹¶ç”Ÿæˆæ•°æ®
    data = load_sources()
    print("[OK] sources.yml loaded")
    category_data, all_domains, stats = generate_data(data)

    # 3. ä¿å­˜è¿™æ¬¡çš„ç»Ÿè®¡æ•°æ®ï¼ˆä¾›ä¸‹æ¬¡å¯¹æ¯”ï¼‰
    save_stats(stats)
    print("[OK] Current stats saved")

    # 4. å†™å…¥ DNS æ–‡ä»¶å’Œ README
    write_dns(category_data)
    write_readme(all_domains, category_data, prev_stats)
    print(f"=== Done: {len(all_domains)} domains ===")


if __name__ == "__main__":
    main()
