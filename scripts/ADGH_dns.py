import yaml
import requests
from datetime import datetime
import pytz
import tldextract
import os

# è„šæœ¬ç°åœ¨åœ¨ scripts/ æ–‡ä»¶å¤¹é‡Œè¿è¡Œï¼Œæ‰€æœ‰è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„éƒ½ç›¸å¯¹äº scripts/
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
SOURCE_FILE = os.path.join(SCRIPT_DIR, "sources.yml")
OUTPUT_DNS = os.path.join(SCRIPT_DIR, "..", "adguard_dns.txt")       # è¾“å‡ºåˆ°æ ¹ç›®å½•
OUTPUT_README = os.path.join(SCRIPT_DIR, "..", "README.md")        # è¾“å‡ºåˆ°æ ¹ç›®å½•

extractor = tldextract.TLDExtract(suffix_list_urls=None)


def normalize_domain(domain):
    domain = domain.strip().lower()
    domain = domain.lstrip("+.").lstrip("*.")
    ext = extractor(domain)
    if not ext.domain or not ext.suffix:
        return None
    return f"{ext.domain}.{ext.suffix}"


def fetch_domains(url):
    domains = set()
    print(f"[FETCH] {url}")
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        for line in r.text.splitlines():
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            if line.startswith("domain:"):
                raw = line.replace("domain:", "").strip()
            elif line.startswith("full:"):
                raw = line.replace("full:", "").strip()
            else:
                raw = line
            d = normalize_domain(raw)
            if d:
                domains.add(d)
    except Exception as e:
        print(f"[ERROR] fetch failed: {e}")
    print(f"[OK] got {len(domains)} domains")
    return domains


def chunk_list(lst, size):
    for i in range(0, len(lst), size):
        yield lst[i:i + size]


def load_sources():
    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def generate_data(data):
    category_data = {}
    all_domains = set()
    stats = {}

    for category, cfg in data.get("categories", {}).items():
        dns = cfg.get("dns", "")
        urls = cfg.get("urls", [])
        raw_domains = set()

        for url in urls:
            raw_domains.update(fetch_domains(url))

        # ä¸å†è¿›è¡Œ DNS éªŒè¯ï¼Œæ‰€æœ‰åŸå§‹åŸŸåéƒ½è§†ä¸ºå­˜æ´»
        alive_domains = raw_domains

        category_data[category] = {
            "dns": dns,
            "domains": sorted(alive_domains),
            "raw_count": len(raw_domains),
            "alive_count": len(alive_domains),
        }
        stats[category] = len(alive_domains)
        all_domains.update(alive_domains)

        print(f"[FILTER] {category}: {len(raw_domains)} â†’ {len(alive_domains)}")

    return category_data, all_domains, stats


def write_dns(category_data):
    with open(OUTPUT_DNS, "w", encoding="utf-8") as f:
        # å®Œå…¨ä¸å†™ä»»ä½•å¤´éƒ¨æ³¨é‡Šï¼Œç›´æ¥å¼€å§‹å†™è§„åˆ™
        for category, info in category_data.items():
            domains = info["domains"]
            dns = info["dns"]
            if not domains:
                continue

            # ä¸å†™ "# {category} ({len(domains)} domains)\n" è¿™è¡Œ
            for chunk in chunk_list(domains, 200):  # æˆ–ä½ å–œæ¬¢çš„æ•°å­—ï¼Œæ¯”å¦‚ 60/100/200
                merged = "/".join(chunk)
                f.write(f"[/{merged}/]{dns}\n")

            # å¯ä»¥é€‰æ‹©åœ¨ç±»åˆ«ä¹‹é—´åŠ ä¸€ä¸ªç©ºè¡Œï¼Œä¹Ÿå¯ä»¥ä¸åŠ 
            f.write("\n")  # â† è¿™è¡Œå¯é€‰ï¼Œçœ‹ä½ å–œä¸å–œæ¬¢ç±»åˆ«é—´æœ‰åˆ†éš”


def write_readme(all_domains, category_data):
    beijing = pytz.timezone("Asia/Shanghai")
    now = datetime.now(beijing).strftime("%Y-%m-%d %H:%M:%S")
    
    # è®¡ç®—æ€»æ•°ç”¨äºå¾½ç« å±•ç¤º
    total_count = len(all_domains)
    # æ ¼å¼åŒ–æ—¶é—´ç”¨äºå¾½ç«  (å»æ‰ç©ºæ ¼å’Œå†’å·ï¼Œæˆ–è€…åªä¿ç•™æ—¥æœŸ)
    date_badge = datetime.now(beijing).strftime("%Y-%m-%d")

    # æ„å»ºç»Ÿè®¡è¡¨æ ¼çš„è¡Œ
    table_rows = []
    total_raw = 0
    total_filtered = 0

    for cat, info in category_data.items():
        raw = info['raw_count']
        alive = info['alive_count']
        filtered = raw - alive
        
        total_raw += raw
        total_filtered += filtered
        
        table_rows.append(
            f"| {cat} | {alive:,} | {raw:,} | {filtered} |"
        )

    # æ€»è®¡è¡Œ
    table_rows.append(
        f"| **æ€»è®¡** | **{total_count:,}** | **{total_raw:,}** | **{total_filtered}** |"
    )

    content = f"""# ğŸ›¡ï¸ AdGuardHome DNS åˆ†æµè§„åˆ™

![Total Domains](https://img.shields.io/badge/åŸŸåæ€»æ•°-{total_count}-blue?style=flat-square)
![Last Update](https://img.shields.io/badge/æœ€åæ›´æ–°-{date_badge}-green?style=flat-square)

> ğŸ¤– æœ¬æ–‡ä»¶ç”±è„šæœ¬è‡ªåŠ¨ç”Ÿæˆï¼Œç”¨äº AdGuardHome çš„ DNS åˆ†æµé…ç½®ã€‚

---

## ğŸ“Š æ•°æ®ç»Ÿè®¡

| åˆ†ç±» | æœ‰æ•ˆåŸŸå | åŸå§‹æ•°é‡ | è¿‡æ»¤æ•°é‡ |
| :--- | :---: | :---: | :---: |
{chr(10).join(table_rows)}

---

## ğŸ“ ä½¿ç”¨è¯´æ˜

1.  å¤åˆ¶ä»“åº“æ ¹ç›®å½•ä¸‹çš„ `adguard_dns.txt` æ–‡ä»¶å†…å®¹ã€‚
2.  æ‰“å¼€ AdGuardHome ç®¡ç†é¢æ¿ã€‚
3.  è¿›å…¥ **è®¾ç½®** -> **DNS æœåŠ¡**ã€‚
4.  åœ¨ **ä¸Šæ¸¸ DNS æœåŠ¡å™¨** é…ç½®ä¸­ï¼Œæ‰¾åˆ°æˆ–æ–°å»ºå¯¹åº”çš„æœåŠ¡å™¨è§„åˆ™ï¼ˆé€šå¸¸æ˜¯ç‰¹å®šåŸŸåçš„åˆ†æµï¼‰ã€‚
5.  å°†å†…å®¹ç²˜è´´å¹¶ä¿å­˜åº”ç”¨å³å¯ã€‚

---

## â° æ›´æ–°è®°å½•

- **ç”Ÿæˆæ—¶é—´**: {now} (åŒ—äº¬æ—¶é—´)
- **ç”Ÿæˆè„šæœ¬**: `scripts/ADGH_dns.py`
"""
    with open(OUTPUT_README, "w", encoding="utf-8") as f:
        f.write(content)


def main():
    print("=== Program start ===")
    data = load_sources()
    print("[OK] sources.yml loaded")
    category_data, all_domains, stats = generate_data(data)
    write_dns(category_data)
    write_readme(all_domains, category_data)
    print(f"=== Done: {len(all_domains)} domains ===")


if __name__ == "__main__":
    main()
